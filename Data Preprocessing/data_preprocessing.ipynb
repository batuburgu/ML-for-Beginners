{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3ef288-ffa4-478c-a8ac-6b433f015f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0df6bf-4667-49c9-aa96-dc12ec3db1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data set and getting the values\n",
    "dataset = pd.read_csv('Desktop/Datasets/Data.csv')\n",
    "X = dataset.iloc[:, :-1].values #first_row:last_row , first_column:last_column\n",
    "y = dataset.iloc[:, -1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ed9114-2c40-441d-9ea2-1bac5b367988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing data with the average value\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3]) # looks for missing values and calculates the average (1:3 is used for including the columns carrying numerical value)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # will replace the missing values with average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadc5b90-39a4-4669-a863-a2edb5fc75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "#Encoding the indepentent variables\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') #[(kind_of_transformation, class_proceeding_encoding, indexes of the columns)]\n",
    "X = np.array(ct.fit_transform(X)) # as arrays are requested in numpy arrays while training models\n",
    "\n",
    "#Encoding dependent variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d5d9f9-00fe-4a5f-8851-2d6f39a4ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1) #(matrix_of_features, dependent_variable_vector, split_size, arbitrary random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24f845c-5558-4269-86a8-56ba62ec34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Since standardisation is resourceful in almost all cases and normalisation is generally better for datasets fitting Gauss Distribution, using standardisation is a safer option.\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # will calculate the mean and the standard deviation and transforms the X_train with standardization\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:]) # For protecting the integrity of the data, the standardization of X_train must be applied to X_test instead of standardizing X_test in itself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
